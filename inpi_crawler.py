"""
INPI Crawler v29.0 - COMPLETO COM LOGIN E BUSCA B√ÅSICA

Baseado em an√°lise completa dos HTMLs reais do INPI:
- 1-login.html: Form POST com T_Login, T_Senha
- 2-escolher-Patente.html: Link para patentes
- 3-search-b√°sico.html: Form POST com ExpressaoPesquisa, Coluna, Action
- 4-escolher-resultados.html: Parse links de resultados
- 5-Resultado-final-da-busca.html: Parse completo patente
- 6-Erro-de-busca.html: "Nenhum resultado foi encontrado"

Fluxo CORRETO:
1. Login ‚Üí /pePI/servlet/LoginController (POST)
2. Patentes ‚Üí /pePI/jsp/patentes/PatenteSearchBasico.jsp (GET)
3. Busca ‚Üí /pePI/servlet/PatenteServletController (POST)
4. Resultados ‚Üí Parse <a href='...Action=detail...'>
5. Detalhes ‚Üí Parse campos completos

Features:
‚úÖ Login COM credenciais (dnm48)
‚úÖ Sess√£o persistente (mant√©m cookies/context)
‚úÖ Busca B√ÅSICA (n√£o avan√ßada!)
‚úÖ Timeout din√¢mico (180s - INPI √© MUITO lento!)
‚úÖ Retry autom√°tico em session expired
‚úÖ Parse completo de cada patente
‚úÖ M√∫ltiplas buscas (T√≠tulo + Resumo)
‚úÖ Tradu√ß√£o PT via Groq AI
"""

import asyncio
import logging
import re
import httpx
from typing import List, Dict, Set, Optional
from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from bs4 import BeautifulSoup

logger = logging.getLogger("pharmyrus")


class INPICrawler:
    """INPI Brazilian Patent Office Crawler - COMPLETE with LOGIN"""
    
    def __init__(self):
        self.found_brs: Set[str] = set()
        self.session_active = False
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
    async def search_inpi(
        self,
        molecule: str,
        brand: str,
        dev_codes: List[str],
        groq_api_key: str,
        username: str = "dnm48",
        password: str = "coresxxx"
    ) -> List[Dict]:
        """
        Search INPI with LOGIN - COMPLETE FLOW
        
        Args:
            molecule: Molecule name (English)
            brand: Brand name (English)  
            dev_codes: Development codes
            groq_api_key: Groq API key for Portuguese translation
            username: INPI login
            password: INPI password
        
        Returns:
            List of BR patents found
        """
        all_patents = []
        
        # Translate to Portuguese using Groq
        logger.info("====================================================================================================")
        
        molecule_pt, brand_pt = await self._translate_to_portuguese(
            molecule, brand, groq_api_key
        )
        
        logger.info(f"   ‚úÖ Translations:")
        logger.info(f"      Molecule: {molecule} ‚Üí {molecule_pt}")
        if brand:
            logger.info(f"      Brand: {brand} ‚Üí {brand_pt}")
        
        # Build search terms (INCLUINDO brand_pt + EN + CAS!)
        search_terms = self._build_search_terms(
            molecule=molecule_pt,
            brand=brand_pt,
            dev_codes=dev_codes,
            max_terms=25,  # v29.2: expandido
            molecule_en=molecule,  # v29.2: NOVO
            brand_en=brand,        # v29.2: NOVO
            cas_number=dev_codes[0] if dev_codes and dev_codes[0].count('-') == 2 else None  # v29.2: NOVO - tenta extrair CAS
        )
        
        logger.info(f"   üìã {len(search_terms)} search terms generated")
        
        # v29.5: DEBUG - Mostrar primeiras 10 queries
        logger.info("   üîç v29.5 DEBUG - Primeiras 10 queries:")
        for idx, term in enumerate(search_terms[:10], 1):
            logger.info(f"      {idx}. '{term}'")
        
        logger.info(f"   üîê Starting INPI search with LOGIN ({username})...")
        
        try:
            async with async_playwright() as p:
                # STEP 0: Launch browser with stealth (MANT√âM SESS√ÉO!)
                self.browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-setuid-sandbox'
                    ]
                )
                
                self.context = await self.browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    locale='pt-BR'
                )
                
                self.page = await self.context.new_page()
                
                # STEP 1: LOGIN
                login_success = await self._login(username, password)
                
                if not login_success:
                    logger.error("   ‚ùå LOGIN failed!")
                    await self.browser.close()
                    return all_patents
                
                logger.info("   ‚úÖ LOGIN successful!")
                self.session_active = True
                
                # STEP 2: Navigate to Patents Basic Search
                try:
                    await self.page.goto(
                        "https://busca.inpi.gov.br/pePI/jsp/patentes/PatenteSearchBasico.jsp",
                        wait_until='networkidle',
                        timeout=180000  # 3 minutes!
                    )
                    logger.info("   üìÑ Patent search page loaded")
                except Exception as e:
                    logger.error(f"   ‚ùå Error loading search page: {str(e)}")
                    await self.browser.close()
                    return all_patents
                
                # STEP 3: Search each term (T√çTULO + RESUMO)
                for i, term in enumerate(search_terms, 1):
                    logger.info(f"   üîç INPI search {i}/{len(search_terms)}: '{term}'")
                    
                    # v29.8: RE-LOGIN PREVENTIVO a cada 4 buscas! (n√£o 5)
                    if i > 1 and (i - 1) % 4 == 0:
                        logger.info(f"   üîÑ Query #{i}: RE-LOGIN preventivo (a cada 4 buscas)")
                        try:
                            relogin = await self._login(username, password)
                            if relogin:
                                logger.info("   ‚úÖ Re-login preventivo OK!")
                                self.session_active = True
                                
                                # Voltar para p√°gina de busca
                                await self.page.goto(
                                    "https://busca.inpi.gov.br/pePI/jsp/patentes/PatenteSearchBasico.jsp",
                                    wait_until='networkidle',
                                    timeout=180000
                                )
                                await asyncio.sleep(2)
                            else:
                                logger.warning("   ‚ö†Ô∏è  Re-login preventivo falhou, continuando mesmo assim...")
                        except Exception as e:
                            logger.warning(f"   ‚ö†Ô∏è  Erro no re-login preventivo: {e}, continuando...")
                    
                    # v30.2: Retry inteligente com re-login
                    max_retries = 2
                    for attempt in range(max_retries):
                        try:
                            # Search by T√çTULO
                            patents_titulo = await self._search_term_basic(term, field="Titulo")
                            all_patents.extend(patents_titulo)
                            
                            await asyncio.sleep(3)  # Delay between searches
                            
                            # Search by RESUMO
                            patents_resumo = await self._search_term_basic(term, field="Resumo")
                            all_patents.extend(patents_resumo)
                            
                            await asyncio.sleep(3)
                            
                            # v30.2: Se chegou aqui, sucesso! Sair do loop
                            break
                            
                        except Exception as e:
                            logger.warning(f"      ‚ö†Ô∏è  Error searching '{term}' (attempt {attempt + 1}/{max_retries}): {str(e)}")
                            
                            if attempt < max_retries - 1:
                                # v30.2: RE-LOGIN IMEDIATO e RETRY!
                                logger.warning(f"      üîÑ RE-LOGIN IMEDIATO devido a erro!")
                                
                                relogin = await self._login(username, password)
                                if relogin:
                                    logger.info("      ‚úÖ Re-login OK! Retrying query...")
                                    self.session_active = True
                                    
                                    # Voltar para p√°gina de busca
                                    await self.page.goto(
                                        "https://busca.inpi.gov.br/pePI/jsp/patentes/PatenteSearchBasico.jsp",
                                        wait_until='networkidle',
                                        timeout=180000
                                    )
                                    await asyncio.sleep(2)
                                    # Loop vai tentar novamente
                                else:
                                    logger.error("      ‚ùå Re-login failed! Skipping query...")
                                    break  # Pula esta query
                            else:
                                # √öltima tentativa falhou
                                logger.error(f"      ‚ùå Query '{term}' failed after {max_retries} attempts")
                
                await self.browser.close()
                
        except Exception as e:
            logger.error(f"   ‚ùå INPI crawler fatal error: {str(e)}")
            if self.browser:
                await self.browser.close()
        
        # Deduplicate
        unique_patents = []
        seen_numbers = set()
        for patent in all_patents:
            num = patent["patent_number"]
            if num not in seen_numbers:
                unique_patents.append(patent)
                seen_numbers.add(num)
        
        if unique_patents:
            logger.info(f"   ‚úÖ INPI search SUCCESS: {len(unique_patents)} BRs found!")
        else:
            logger.warning("   ‚ö†Ô∏è  INPI search returned 0 results")
        
        return unique_patents
    
    async def _login(self, username: str, password: str) -> bool:
        """
        STEP 1: Perform LOGIN on INPI
        
        Based on 1-login.html:
        - URL: https://busca.inpi.gov.br/pePI/
        - Form POST to: /pePI/servlet/LoginController
        - Fields: T_Login, T_Senha
        - Hidden: action=login
        
        Returns:
            True if login successful
        """
        try:
            logger.info("   üìù Accessing login page...")
            
            # Go to login page
            await self.page.goto(
                "https://busca.inpi.gov.br/pePI/",
                wait_until='networkidle',
                timeout=60000  # 1 min
            )
            
            await asyncio.sleep(2)
            
            logger.info(f"   üîë Logging in as {username}...")
            
            # Fill login form
            await self.page.fill('input[name="T_Login"]', username, timeout=20000)
            await self.page.fill('input[name="T_Senha"]', password, timeout=20000)
            
            await asyncio.sleep(1)
            
            # Click Continue button (value contains "Continuar")
            await self.page.click('input[type="submit"][value*="Continuar"]', timeout=20000)
            
            # Wait for navigation
            await self.page.wait_for_load_state('networkidle', timeout=60000)
            
            await asyncio.sleep(2)
            
            # Check if login was successful
            content = await self.page.content()
            
            # Success indicators:
            # - "Login: dnm48" appears in page
            # - "Patente" link available
            # - "Finalizar Sess√£o" link available
            
            if username.lower() in content.lower() or "Finalizar Sess" in content or "patente" in content.lower():
                logger.info(f"   ‚úÖ Login successful! Session active")
                return True
            else:
                logger.error("   ‚ùå Login failed - no session indicators found")
                return False
                
        except Exception as e:
            logger.error(f"   ‚ùå Login error: {str(e)}")
            return False
    
    async def _search_term_basic(
        self,
        term: str,
        field: str = "Titulo"
    ) -> List[Dict]:
        """
        STEP 3: Search a single term using BASIC search
        
        Based on 3-search-b√°sico.html:
        - Form POST to: /pePI/servlet/PatenteServletController
        - Fields:
          * ExpressaoPesquisa = search term
          * Coluna = "Titulo" or "Resumo"
          * FormaPesquisa = "todasPalavras"
          * RegisterPerPage = "100"
          * Action = "SearchBasico"
        
        Args:
            term: Search term
            field: "Titulo" or "Resumo"
        
        Returns:
            List of BR patents found
        """
        results = []
        
        try:
            # Make sure we're on search page
            current_url = self.page.url
            if "PatenteSearchBasico.jsp" not in current_url:
                await self.page.goto(
                    "https://busca.inpi.gov.br/pePI/jsp/patentes/PatenteSearchBasico.jsp",
                    wait_until='networkidle',
                    timeout=60000  # 1 min
                )
                await asyncio.sleep(2)
            
            # Fill search form
            await self.page.fill('input[name="ExpressaoPesquisa"]', term, timeout=20000)
            
            # Select field (Titulo or Resumo) with timeout
            await self.page.select_option('select[name="Coluna"]', field, timeout=20000)
            
            # Select "todas as palavras"
            await self.page.select_option('select[name="FormaPesquisa"]', 'todasPalavras', timeout=20000)
            
            # Select 100 results per page
            await self.page.select_option('select[name="RegisterPerPage"]', '100', timeout=20000)
            
            await asyncio.sleep(1)
            
            # Click Search button
            await self.page.click('input[type="submit"][name="botao"]', timeout=20000)
            
            # Wait for results (shorter timeout)
            await self.page.wait_for_load_state('networkidle', timeout=60000)
            
            await asyncio.sleep(2)
            
            # Get page content
            content = await self.page.content()
            
            # Check for "Nenhum resultado" (no results)
            if "Nenhum resultado foi encontrado" in content:
                logger.info(f"      ‚ö†Ô∏è  No results for '{term}' in {field}")
                return results
            
            # Parse results
            soup = BeautifulSoup(content, 'html.parser')
            
            # Find all BR patent links
            # Pattern from 4-escolher-resultados.html:
            # <a href='/pePI/servlet/PatenteServletController?Action=detail&CodPedido=1748765...'>BR 11 2024 016586 8</a>
            
            patent_links = soup.find_all('a', href=re.compile(r'Action=detail'))
            
            if patent_links:
                logger.info(f"      ‚úÖ Found {len(patent_links)} result(s) for '{term}' in {field}")
            
            # First pass: collect all BR numbers and their detail URLs
            br_details_to_fetch = []
            for link in patent_links:
                try:
                    br_text = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    # Extract BR number: "BR 11 2024 016586 8" -> "BR112024016586"
                    br_clean = re.sub(r'\s+', '', br_text)
                    match = re.search(r'(BR[A-Z]*\d+)', br_clean)
                    
                    if match:
                        br_number = match.group(1)
                        if br_number not in self.found_brs:
                            self.found_brs.add(br_number)
                            
                            # Build full URL
                            if href.startswith('/'):
                                detail_url = f"https://busca.inpi.gov.br{href}"
                            else:
                                detail_url = href
                            
                            br_details_to_fetch.append({
                                'br_number': br_number,
                                'url': detail_url
                            })
                except Exception as e:
                    logger.warning(f"      ‚ö†Ô∏è  Error parsing link: {e}")
                    continue
            
            # Second pass: fetch details for each BR
            for item in br_details_to_fetch:
                br_number = item['br_number']
                detail_url = item['url']
                
                try:
                    logger.info(f"         ‚Üí {br_number} - Fetching details...")
                    
                    # Navigate to detail page
                    await self.page.goto(detail_url, wait_until='networkidle', timeout=60000)
                    await asyncio.sleep(2)
                    
                    # Parse complete details
                    details = await self._parse_patent_details(br_number)
                    if details and details.get('patent_number'):
                        details['source'] = 'INPI'
                        details['search_term'] = term
                        details['search_field'] = field
                        results.append(details)
                        logger.info(f"            ‚úÖ Parsed {sum([1 for v in details.values() if v])} fields")
                    else:
                        # Fallback: add minimal data
                        results.append({
                            "patent_number": br_number,
                            "country": "BR",
                            "source": "INPI",
                            "search_term": term,
                            "search_field": field
                        })
                        logger.warning(f"            ‚ö†Ô∏è  Minimal data only")
                    
                except Exception as e:
                    logger.error(f"            ‚ùå Error fetching details: {e}")
                    # Fallback: add minimal data
                    results.append({
                        "patent_number": br_number,
                        "country": "BR",
                        "source": "INPI",
                        "search_term": term,
                        "search_field": field
                    })
            
        except Exception as e:
            logger.error(f"      ‚ùå Error in basic search: {str(e)}")
        
        return results
    
    async def _check_session_expired(self) -> bool:
        """
        Check if INPI session has expired
        
        Returns:
            True if session expired (redirected to login)
        """
        try:
            current_url = self.page.url
            content = await self.page.content()
            
            # Session expired if:
            # - URL contains "login"
            # - Content has login form
            
            if "login" in current_url.lower() or "T_Login" in content:
                return True
            
            return False
            
        except:
            return False
    
    async def _parse_patent_details(self, br_number: str) -> Dict:
        """
        Parse COMPLETE patent details from INPI detail page
        Extracts ALL 18+ fields based on real INPI HTML structure
        
        Fields extracted:
        - (21) Patent Number
        - (22) Filing Date  
        - (43) Publication Date
        - (47) Grant Date
        - (30) Priority Data (multiple)
        - (51) IPC Codes
        - (54) Title
        - (57) Abstract
        - (71) Applicants
        - (72) Inventors
        - (74) Attorney
        - (85) National Phase Date
        - (86) PCT Number & Date
        - (87) WO Number & Date
        - Anuidades (fee schedule)
        - Despachos (RPI publications)
        - Documents & PDF links
        """
        try:
            content = await self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            details = {
                'patent_number': br_number,
                'country': 'BR',
                'title': None,
                'title_original': None,
                'abstract': None,
                'applicants': [],
                'inventors': [],
                'ipc_codes': [],
                'publication_date': None,
                'filing_date': None,
                'grant_date': None,
                'priority_data': [],
                'pct_number': None,
                'pct_date': None,
                'wo_number': None,
                'wo_date': None,
                'national_phase_date': None,
                'attorney': None,
                'anuidades': [],
                'despachos': [],
                'documents': [],
                'pdf_links': [],
                'link_national': self.page.url
            }
            
            # Helper function to parse BR dates DD/MM/YYYY ‚Üí YYYY-MM-DD
            def parse_br_date(date_str):
                if not date_str or date_str.strip() in ['-', '']:
                    return None
                match = re.search(r'(\d{2})/(\d{2})/(\d{4})', date_str)
                if match:
                    day, month, year = match.groups()
                    return f"{year}-{month}-{day}"
                return None
            
            # (22) Filing Date - Data do Dep√≥sito
            filing_tag = soup.find('font', class_='normal', string=re.compile(r'Data do Dep√≥sito:'))
            if filing_tag:
                tr = filing_tag.find_parent('tr')
                if tr:
                    tds = tr.find_all('td')
                    if len(tds) >= 2:
                        date_text = tds[1].get_text(strip=True)
                        details['filing_date'] = parse_br_date(date_text)
            
            # (43) Publication Date - Data da Publica√ß√£o
            pub_tag = soup.find('font', class_='normal', string=re.compile(r'Data da Publica√ß√£o:'))
            if pub_tag:
                tr = pub_tag.find_parent('tr')
                if tr:
                    tds = tr.find_all('td')
                    if len(tds) >= 2:
                        date_text = tds[1].get_text(strip=True)
                        details['publication_date'] = parse_br_date(date_text)
            
            # (47) Grant Date - Data da Concess√£o
            grant_tag = soup.find('font', class_='normal', string=re.compile(r'Data da Concess√£o:'))
            if grant_tag:
                tr = grant_tag.find_parent('tr')
                if tr:
                    tds = tr.find_all('td')
                    if len(tds) >= 2:
                        date_text = tds[1].get_text(strip=True)
                        if date_text and date_text != '-':
                            details['grant_date'] = parse_br_date(date_text)
            
            # (30) Priority Data - Find priority table
            priority_section = soup.find('font', class_='alerta', string=re.compile(r'\(30\)'))
            if priority_section:
                # Find next table after (30)
                current = priority_section
                for _ in range(10):  # Search up to 10 siblings
                    if current is None:
                        break
                    current = current.find_next_sibling()
                    if current and current.name == 'table':
                        rows = current.find_all('tr')[1:]  # Skip header
                        for row in rows:
                            cols = row.find_all('td')
                            if len(cols) >= 3:
                                country = cols[0].get_text(strip=True)
                                number = cols[1].get_text(strip=True)
                                date = cols[2].get_text(strip=True)
                                if country and number:
                                    details['priority_data'].append({
                                        'country': country,
                                        'number': number,
                                        'date': parse_br_date(date)
                                    })
                        break
            
            # (51) IPC Classification
            ipc_tag = soup.find('font', class_='alerta', string=re.compile(r'\(51\)'))
            if ipc_tag:
                tr = ipc_tag.find_parent('tr')
                if tr:
                    # Get all text and split by semicolon/newline
                    ipc_text = tr.get_text()
                    for code in re.split(r'[;\n]', ipc_text):
                        code = code.strip()
                        # Filter out non-IPC text
                        if code and not code.startswith('(') and not 'Classifica√ß√£o' in code:
                            # Match IPC pattern: letter + numbers
                            if re.match(r'[A-H]\d', code):
                                details['ipc_codes'].append(code)
            
            # (54) Title - T√≠tulo
            title_tag = soup.find('font', class_='alerta', string=re.compile(r'\(54\)'))
            if title_tag:
                tr = title_tag.find_parent('tr')
                if tr:
                    # Try div first (modern INPI)
                    title_div = tr.find('div', id='tituloContext')
                    if title_div:
                        title_text = title_div.get_text(strip=True)
                    else:
                        # Fallback: next td after (54)
                        tds = tr.find_all('td')
                        if len(tds) >= 2:
                            title_text = tds[1].get_text(strip=True)
                        else:
                            title_text = tr.get_text(strip=True).replace('(54)', '').replace('T√≠tulo:', '').strip()
                    
                    if title_text:
                        details['title'] = title_text
                        details['title_original'] = title_text
            
            # (57) Abstract - Resumo
            abstract_tag = soup.find('font', class_='alerta', string=re.compile(r'\(57\)'))
            if abstract_tag:
                tr = abstract_tag.find_parent('tr')
                if tr:
                    # Try div first (modern INPI)
                    abstract_div = tr.find('div', id='resumoContext')
                    if abstract_div:
                        abstract_text = abstract_div.get_text(strip=True)
                    else:
                        # Fallback: next td after (57)
                        tds = tr.find_all('td')
                        if len(tds) >= 2:
                            abstract_text = tds[1].get_text(strip=True)
                        else:
                            abstract_text = tr.get_text(strip=True).replace('(57)', '').replace('Resumo:', '').strip()
                    
                    if abstract_text:
                        details['abstract'] = abstract_text
            
            # (71) Applicants - Nome do Depositante
            applicant_tag = soup.find('font', class_='alerta', string=re.compile(r'\(71\)'))
            if applicant_tag:
                tr = applicant_tag.find_parent('tr')
                if tr:
                    applicant_text = tr.get_text(strip=True)
                    applicant_text = applicant_text.replace('(71)', '').replace('Nome do Depositante:', '').strip()
                    # Split by / for multiple applicants
                    if applicant_text:
                        details['applicants'] = [a.strip() for a in applicant_text.split('/') if a.strip()]
            
            # (72) Inventors - Nome do Inventor
            inventor_tag = soup.find('font', class_='alerta', string=re.compile(r'\(72\)'))
            if inventor_tag:
                tr = inventor_tag.find_parent('tr')
                if tr:
                    inventor_text = tr.get_text(strip=True)
                    inventor_text = inventor_text.replace('(72)', '').replace('Nome do Inventor:', '').strip()
                    # Split by / for multiple inventors
                    if inventor_text:
                        details['inventors'] = [i.strip() for i in inventor_text.split('/') if i.strip()]
            
            # (74) Attorney - Nome do Procurador
            attorney_tag = soup.find('font', class_='alerta', string=re.compile(r'\(74\)'))
            if attorney_tag:
                tr = attorney_tag.find_parent('tr')
                if tr:
                    attorney_text = tr.get_text(strip=True)
                    details['attorney'] = attorney_text.replace('(74)', '').replace('Nome do Procurador:', '').strip()
            
            # (85) National Phase Entry Date
            phase_tag = soup.find('font', class_='alerta', string=re.compile(r'\(85\)'))
            if phase_tag:
                tr = phase_tag.find_parent('tr')
                if tr:
                    phase_text = tr.get_text(strip=True)
                    date_match = re.search(r'(\d{2}/\d{2}/\d{4})', phase_text)
                    if date_match:
                        details['national_phase_date'] = parse_br_date(date_match.group(1))
            
            # (86) PCT Number and Date
            pct_tag = soup.find('font', class_='alerta', string=re.compile(r'\(86\)'))
            if pct_tag:
                tr = pct_tag.find_parent('tr')
                if tr:
                    pct_text = tr.get_text(strip=True)
                    # Extract PCT number (e.g., EP2023054766)
                    pct_match = re.search(r'([A-Z]{2}\d{10,})', pct_text)
                    if pct_match:
                        details['pct_number'] = pct_match.group(1)
                    # Extract date
                    date_match = re.search(r'Data[:\s]*(\d{2}/\d{2}/\d{4})', pct_text)
                    if date_match:
                        details['pct_date'] = parse_br_date(date_match.group(1))
            
            # (87) WO Number and Date
            wo_tag = soup.find('font', class_='alerta', string=re.compile(r'\(87\)'))
            if wo_tag:
                tr = wo_tag.find_parent('tr')
                if tr:
                    wo_text = tr.get_text(strip=True)
                    # Extract WO number (e.g., 2023/161458)
                    wo_match = re.search(r'(\d{4})/(\d{6})', wo_text)
                    if wo_match:
                        details['wo_number'] = f"WO{wo_match.group(1)}{wo_match.group(2)}"
                    # Extract date
                    date_match = re.search(r'Data[:\s]*(\d{2}/\d{2}/\d{4})', wo_text)
                    if date_match:
                        details['wo_date'] = parse_br_date(date_match.group(1))
            
            # Anuidades (Fee Schedule) - Find table with "Ordin√°rio" and "Extraordin√°rio"
            for table in soup.find_all('table'):
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        fee_type = cells[0].get_text(strip=True)
                        if fee_type in ['Ordin√°rio', 'Extraordin√°rio']:
                            # Get date range from next cells
                            dates = []
                            for cell in cells[1:]:
                                date_text = cell.get_text(strip=True)
                                if date_text and '/' in date_text:
                                    dates.append(date_text)
                            if dates:
                                details['anuidades'].append({
                                    'type': fee_type,
                                    'dates': ' - '.join(dates)
                                })
            
            # Despachos (Publications in RPI) - Find table with RPI numbers
            pub_table = soup.find('div', id='accordionPublicacoes')
            if pub_table:
                rows = pub_table.find_all('tr', class_='normal')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 3:
                        rpi = cells[0].get_text(strip=True)
                        rpi_date = cells[1].get_text(strip=True)
                        despacho_code = cells[2].get_text(strip=True)
                        
                        # Check for PDF link
                        pdf_link = None
                        if len(cells) > 3:
                            img = cells[3].find('img')
                            if img:
                                pdf_link = f"https://busca.inpi.gov.br/pePI/servlet/PatenteServletController?Action=detail&CodPedido={br_number}&RPI={rpi}"
                        
                        details['despachos'].append({
                            'rpi': rpi,
                            'rpi_date': parse_br_date(rpi_date),
                            'despacho_code': despacho_code,
                            'pdf_link': pdf_link
                        })
            
            # PDF Links from Document Section
            doc_section = soup.find('div', class_='scroll-content')
            if doc_section:
                images = doc_section.find_all('img')
                for img in images:
                    img_id = img.get('id', '')
                    label = img.find_next('label')
                    if label:
                        rpi_text = label.get_text(strip=True)
                        pdf_url = f"https://busca.inpi.gov.br/pePI/servlet/PatenteServletController?Action=detail&CodPedido={br_number}"
                        details['pdf_links'].append({
                            'rpi': rpi_text,
                            'document_id': img_id,
                            'pdf_url': pdf_url
                        })
            
            # Count extracted fields
            fields_count = sum([
                1 if details['title'] else 0,
                1 if details['abstract'] else 0,
                1 if details['filing_date'] else 0,
                1 if details['publication_date'] else 0,
                1 if details['applicants'] else 0,
                1 if details['inventors'] else 0,
                1 if details['ipc_codes'] else 0,
                1 if details['priority_data'] else 0,
                1 if details['pct_number'] else 0,
                1 if details['wo_number'] else 0,
                1 if details['attorney'] else 0,
                1 if details['anuidades'] else 0,
                1 if details['despachos'] else 0,
                1 if details['pdf_links'] else 0,
            ])
            
            logger.info(f"         ‚úÖ Extracted {fields_count} fields for {br_number}")
            return details
            
        except Exception as e:
            logger.error(f"         ‚ùå Error parsing details for {br_number}: {e}")
            import traceback
            traceback.print_exc()
            return {'patent_number': br_number, 'country': 'BR'}
    
    async def search_by_numbers(self, br_numbers: List[str], username: str, password: str) -> List[Dict]:
        """
        Search INPI by patent numbers using ADVANCED SEARCH
        Used to enrich BR patents found via EPO
        
        IMPORTANT: Must use Advanced Search page because Basic Search doesn't have "Number" field!
        URL: https://busca.inpi.gov.br/pePI/jsp/patentes/PatenteSearchAvancado.jsp
        Field: NumPedido (21) N¬∫ do Pedido
        """
        if not br_numbers:
            return []
        
        logger.info(f"üîç INPI: Searching {len(br_numbers)} BRs by number (ADVANCED SEARCH)")
        all_patents = []
        
        try:
            async with async_playwright() as p:
                self.browser = await p.chromium.launch(headless=True)
                self.context = await self.browser.new_context()
                self.page = await self.context.new_page()
                
                # Login
                login_ok = await self._login(username, password)
                if not login_ok:
                    logger.error("‚ùå Login failed for number search")
                    return []
                
                # Search each BR by number using ADVANCED SEARCH
                for i, br_number in enumerate(br_numbers, 1):
                    max_retries = 2  # v30.5: Retry timeout errors
                    retry_delay = 3  # seconds
                    
                    for attempt in range(max_retries):
                        try:
                            if attempt > 0:
                                logger.warning(f"      üîÑ Retry {attempt}/{max_retries-1} for {br_number}")
                                await asyncio.sleep(retry_delay * attempt)  # Backoff exponencial
                            
                            logger.info(f"   üìÑ {i}/{len(br_numbers)}: {br_number}")
                            
                            # Format BR number for search (keep as is)
                            search_term = br_number.strip()
                            
                            # Go to ADVANCED search page
                            await self.page.goto(
                                "https://busca.inpi.gov.br/pePI/jsp/patentes/PatenteSearchAvancado.jsp",
                                wait_until='networkidle',
                                timeout=30000
                            )
                            await asyncio.sleep(1)
                            
                            # Fill NumPedido field (21) - Patent Number
                            await self.page.fill('input[name="NumPedido"]', search_term, timeout=20000)
                            
                            # Click Search button
                            await self.page.click('input[type="submit"][name="botao"]', timeout=20000)
                            await self.page.wait_for_load_state('networkidle', timeout=30000)
                            await asyncio.sleep(2)
                            
                            # Check results
                            content = await self.page.content()
                            
                            if "Nenhum resultado foi encontrado" in content:
                                logger.warning(f"      ‚ö†Ô∏è  No results found for {br_number}")
                                break  # N√£o retry se n√£o tem resultado
                            
                            # Find and click detail link
                            if "Action=detail" in content:
                                soup = BeautifulSoup(content, 'html.parser')
                                first_link = soup.find('a', href=re.compile(r'Action=detail'))
                                
                                if first_link:
                                    # Click to go to detail page
                                    await self.page.click(f'a[href*="Action=detail"]', timeout=20000)
                                    await self.page.wait_for_load_state('networkidle', timeout=30000)
                                    await asyncio.sleep(2)
                                    
                                    # Parse details
                                    details = await self._parse_patent_details(br_number)
                                    if details and details.get('patent_number'):
                                        details['source'] = 'INPI'
                                        all_patents.append(details)
                                        logger.info(f"      ‚úÖ Got details for {br_number}")
                                    else:
                                        logger.warning(f"      ‚ö†Ô∏è  Could not parse details for {br_number}")
                                else:
                                    logger.warning(f"      ‚ö†Ô∏è  Could not find detail link for {br_number}")
                            else:
                                logger.warning(f"      ‚ö†Ô∏è  No detail link in results for {br_number}")
                            
                            # Sucesso - sair do loop de retry
                            break
                            
                        except Exception as e:
                            error_msg = str(e)
                            is_timeout = "Timeout" in error_msg or "timeout" in error_msg.lower()
                            
                            if is_timeout and attempt < max_retries - 1:
                                logger.warning(f"      ‚è±Ô∏è  Timeout on attempt {attempt+1}/{max_retries} for {br_number}")
                                continue  # Retry
                            else:
                                # √öltima tentativa ou erro n√£o-timeout
                                logger.error(f"      ‚ùå Error searching {br_number}: {error_msg}")
                                if attempt == max_retries - 1:
                                    logger.error(f"      ‚ùå Failed after {max_retries} attempts")
                                break
                        
                        await asyncio.sleep(2)  # Rate limit between searches
                
                await self.browser.close()
                
        except Exception as e:
            logger.error(f"‚ùå Error in number search: {e}")
        
        logger.info(f"‚úÖ INPI: Got details for {len(all_patents)}/{len(br_numbers)} BRs")
        return all_patents
    
    def _build_search_terms(
        self,
        molecule: str,
        brand: str,
        dev_codes: List[str],
        max_terms: int = 25,  # EXPANDIDO: era 8, agora 25
        molecule_en: str = None,  # NOVO: mol√©cula em ingl√™s
        brand_en: str = None,     # NOVO: brand em ingl√™s
        cas_number: str = None    # NOVO: CAS number
    ) -> List[str]:
        """
        Build search terms from molecule, brand, dev codes
        
        v29.4 FIX: 
        - Mol√©cula PT SEMPRE PRIMEIRA!
        - Ordem garantida (n√£o usa set)
        - Re-login a cada 8 buscas
        
        Args:
            molecule: Molecule name (in Portuguese!)
            brand: Brand name (in Portuguese!)
            dev_codes: Development codes
            max_terms: Maximum number of terms
            molecule_en: NOVO - Molecule name in English
            brand_en: NOVO - Brand name in English
            cas_number: NOVO - CAS registry number
        
        Returns:
            List of search terms (ORDERED!)
        """
        terms = []  # v29.4: Lista ordenada, N√ÉO set!
        seen = set()  # Para deduplica√ß√£o
        
        def add_term(term):
            """Adiciona termo se ainda n√£o foi adicionado"""
            if term and term not in seen:
                terms.append(term.strip())
                seen.add(term.strip())
        
        # 1. MOL√âCULA PT - SEMPRE PRIMEIRA! üáßüá∑
        if molecule:
            add_term(molecule)
        
        # 2. Mol√©cula EN
        if molecule_en and molecule_en != molecule:
            add_term(molecule_en)
        
        # 3. Brand PT
        if brand and brand != molecule:
            add_term(brand)
        
        # 4. Brand EN
        if brand_en and brand_en != brand and brand_en != molecule:
            add_term(brand_en)
        
        # 5. CAS number
        if cas_number:
            add_term(cas_number)
        
        # 6. Dev codes
        for code in dev_codes[:10]:
            if code and len(code) > 2:
                add_term(code)
                
                # Varia√ß√µes sem h√≠fen
                if '-' in code:
                    add_term(code.replace('-', ''))
        
        # 7. Varia√ß√µes sem espa√ßo
        if molecule_en and ' ' in molecule_en:
            add_term(molecule_en.replace(' ', ''))
        
        if molecule and ' ' in molecule:
            add_term(molecule.replace(' ', ''))
        
        # Limitar
        return terms[:max_terms]
    
    async def _translate_to_portuguese(
        self,
        molecule: str,
        brand: str,
        groq_api_key: str
    ) -> tuple:
        """
        Translate molecule and brand to Portuguese using Groq AI
        
        Args:
            molecule: Molecule name in English
            brand: Brand name in English
            groq_api_key: Groq API key
        
        Returns:
            (molecule_pt, brand_pt) tuple
        """
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # Translate molecule
                molecule_pt = await self._groq_translate(client, molecule, groq_api_key)
                
                # Translate brand if different
                if brand and brand.lower() != molecule.lower():
                    brand_pt = await self._groq_translate(client, brand, groq_api_key, is_brand=True)
                else:
                    brand_pt = molecule_pt
                
                return molecule_pt, brand_pt
                
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Translation error: {str(e)}, using original names")
            return molecule, brand
    
    async def _groq_translate(
        self,
        client: httpx.AsyncClient,
        text: str,
        groq_api_key: str,
        is_brand: bool = False
    ) -> str:
        """
        Translate text to Portuguese using Groq
        
        Args:
            client: HTTP client
            text: Text to translate
            groq_api_key: Groq API key
            is_brand: True if translating brand name (uses different prompt)
        
        Returns:
            Translated text in Portuguese
        """
        try:
            if is_brand:
                # Para marcas: buscar nome brasileiro ou manter original
                system_prompt = "You are a pharmaceutical expert. If this brand name has a Brazilian/Portuguese version, return it. Otherwise, return the ORIGINAL name unchanged. Return ONLY the name, nothing else."
                user_prompt = f"What is the Brazilian/Portuguese brand name for: {text}\nIf there is no Brazilian version, return exactly: {text}"
            else:
                # Para mol√©culas: traduzir normalmente
                system_prompt = "You are a pharmaceutical translator. Translate drug molecule names to Portuguese (scientific names). Return ONLY the translated name, nothing else."
                user_prompt = f"Translate this pharmaceutical molecule name to Portuguese: {text}"
            
            response = await client.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {groq_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "llama-3.3-70b-versatile",
                    "messages": [
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": user_prompt
                        }
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                translation = data["choices"][0]["message"]["content"].strip()
                
                # Remove quotes if present
                translation = translation.strip('"').strip("'")
                
                return translation
            else:
                logger.warning(f"   ‚ö†Ô∏è  Groq API error: {response.status_code}")
                return text
                
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Groq translation error: {str(e)}")
            return text


# Singleton instance
inpi_crawler = INPICrawler()
